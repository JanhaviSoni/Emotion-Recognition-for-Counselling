{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Fine Grained Emotion Recognition for Counselling/dreaddit-train.csv')\n",
    "test = pd.read_csv('../Fine Grained Emotion Recognition for Counselling/dreaddit-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = train['text'].values\n",
    "text_test = test['text'].values\n",
    "y_train = train['label'].values\n",
    "y_test = test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "tagger=PerceptronTagger()\n",
    "wordnet = WordNetLemmatizer()\n",
    "corpus_train = []\n",
    "corpus_test =[]\n",
    "tagged = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    review = re.sub('^a-zA-Z^',' ',train['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    tagged.append(tagger.tag(review))\n",
    "    review = ' '.join(review)\n",
    "    corpus_train.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey r/assistance, sure right place post this.. go =) i'm currently student intern sandia national lab working survey help improve marketing outreach effort many school recruit around country. we're looking current undergrad/grad stem student stem student know stem students, would greatly appreciate help take pas along short survey. thank you, everyone help take survey entered drawing chance win one three $50 amazon gcs. \n",
      " \n",
      " mom hit newspaper shocked would this, know like play hitting, smacking, striking, hitting violence sort person. send vibe asking universe? yesterday decided take friend go help another \"friend\" move new place. driving friend moving strike shoulder. address immediately 4th time told things, friend driving nearly get collision another car think high marijuana friend moving backseat like \"you understand trying get attention\" know thing 5 year old get people attention smacking them, guy 60's.\n"
     ]
    }
   ],
   "source": [
    "print(corpus_train[1],'\\n','\\n',corpus_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    review = re.sub('^a-zA-Z^',' ',test['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    tagged.append(tagger.tag(review))\n",
    "    review = ' '.join(review)\n",
    "    corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(corpus_train).toarray()\n",
    "X_test = cv.transform(corpus_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('said', 'VBD') \n",
      " ('felt', 'JJ') \n",
      " ('way', 'NN') \n",
      " ('before,', 'NN') \n",
      " ('suggeted', 'VBD')\n"
     ]
    }
   ],
   "source": [
    "print(tagged[0][0],'\\n',tagged[0][1],'\\n',tagged[0][2],'\\n',tagged[0][3],'\\n',tagged[0][4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using Word2Vec\n",
    "# from gensim.models import Word2Vec\n",
    "# sentences_train = []\n",
    "# sentences_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning data for Word2Vec in training dataset\n",
    "# for i in range(len(train)):\n",
    "#     review = re.sub('^a-zA-Z^',' ',train['text'][i])\n",
    "#     review = review.lower()\n",
    "#     review = review.split()\n",
    "    \n",
    "#     review = [word for word in review if word not in set(stopwords.words('english'))]\n",
    "#     sentences_train.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning data for Word2Vec in testing dataset\n",
    "# for i in range(len(test)):\n",
    "#     review = re.sub('^a-zA-Z^',' ',test['text'][i])\n",
    "#     review = review.lower()\n",
    "#     review = review.split()\n",
    "    \n",
    "#     review = [word for word in review if word not in set(stopwords.words('english'))]\n",
    "#     sentences_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Word2Vec\n",
    "# train_model = Word2Vec(sentences_train, min_count=1)# if word is present less than 1 skip that word\n",
    "# #test_model = Word2Vec(sentences_test, min_count=1)# if word is present less than 1 skip that word\n",
    "# train_model.save(\"word2vec_train.model\")\n",
    "# #test_model.save(\"word2vec_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_train = train_model.build_vocab(sentences_train)\n",
    "# words_test = test_model.build_vocab(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training word2vec\n",
    "# train_model.train(sentences_train,total_examples=train_model.corpus_count,epochs=1001)# corpus_count =2838\n",
    "# #test_model.train(sentences_test,total_examples=test_model.corpus_count,epochs=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Store just the words + their trained embeddings.\n",
    "# word_vectors_train = train_model.wv\n",
    "# word_vectors_train.save(\"word2vec.wordvectors\")\n",
    "# word_vectors_test = train_model.wv\n",
    "# # word_vectors_test.save(\"word2vec.wordvectors\")\n",
    "# # Load back with memory-mapping = read-only, shared across processes.\n",
    "# wv = KeyedVectors.load(\"word2vec_train.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression Model on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state =0)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training K-NN Model on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVM Model on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf' , random_state=0)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Naive Bayes Model on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive = GaussianNB()\n",
    "naive.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Decision Tree Model on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(criterion = 'entropy', random_state =0)\n",
    "dtc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Random Forest Model on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100 , criterion='entropy', random_state=0)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training XGBoost on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:16] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "y_pred_classifier = classifier.predict(X_test)\n",
    "y_pred_naive = naive.predict(X_test)\n",
    "y_pred_dtc = dtc.predict(X_test)\n",
    "y_pred_rfc = rfc.predict(X_test)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "# np.concatenate((y_test.reshape(len(y_test),1),y_pred.reshape(len(y_pred),1)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : \n",
      " [[241 105]\n",
      " [ 85 284]]\n",
      "K-NN : \n",
      " [[231 115]\n",
      " [197 172]]\n",
      "SVM :  [[246 100]\n",
      " [ 97 272]]\n",
      "Naive Bayes : \n",
      " [[123 223]\n",
      " [ 97 272]]\n",
      "Decision Tree Classifier : \n",
      " [[185 161]\n",
      " [130 239]]\n",
      "Random Forest Classifier : \n",
      " [[195 151]\n",
      " [ 60 309]]\n",
      "Accuracy of Logistic Regression Classifier is : 0.7342657342657343 \n",
      "\n",
      "Accuracy of K_NN Classifier is : 0.5636363636363636 \n",
      "\n",
      "Accuracy of SVM Classifier is : 0.7244755244755244 \n",
      "\n",
      "Accuracy of Naive Bayes Classifier is : 0.5524475524475524 \n",
      "\n",
      "Accuracy of Decision Tree Classifier is : 0.593006993006993 \n",
      "\n",
      "Accuracy of Random Forest Classifier is : 0.7048951048951049 \n",
      "\n",
      "Accuracy of XGBoost Classifier is : 0.7048951048951049 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm_lr = confusion_matrix(y_test,y_pred_lr)\n",
    "print('Logistic Regression : \\n',cm_lr)\n",
    "cm_knn = confusion_matrix(y_test,y_pred_knn)\n",
    "print('K-NN : \\n',cm_knn)\n",
    "cm_classifier = confusion_matrix(y_test,y_pred_classifier)\n",
    "print('SVM : ',cm_classifier)\n",
    "cm_naive = confusion_matrix(y_test,y_pred_naive)\n",
    "print('Naive Bayes : \\n',cm_naive)\n",
    "cm_dtc = confusion_matrix(y_test,y_pred_dtc)\n",
    "print('Decision Tree Classifier : \\n',cm_dtc)\n",
    "cm_rfc = confusion_matrix(y_test,y_pred_rfc)\n",
    "print('Random Forest Classifier : \\n', cm_rfc)\n",
    "print('Accuracy of Logistic Regression Classifier is :', accuracy_score(y_test,y_pred_lr),'\\n')\n",
    "print('Accuracy of K_NN Classifier is :', accuracy_score(y_test,y_pred_knn),'\\n')\n",
    "print('Accuracy of SVM Classifier is :' ,accuracy_score(y_test,y_pred_classifier),'\\n')\n",
    "print('Accuracy of Naive Bayes Classifier is :', accuracy_score(y_test,y_pred_naive),'\\n')\n",
    "print('Accuracy of Decision Tree Classifier is :',accuracy_score(y_test,y_pred_dtc),'\\n')\n",
    "print('Accuracy of Random Forest Classifier is :' ,accuracy_score(y_test,y_pred_rfc),'\\n')\n",
    "print('Accuracy of XGBoost Classifier is :' ,accuracy_score(y_test,y_pred_xgb),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Stemming for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "corpus_train_stem = []\n",
    "corpus_test_stem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    review = re.sub('^a-zA-Z^',' ',train['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    tagged.append(tagger.tag(review))\n",
    "    review = ' '.join(review)\n",
    "    corpus_train_stem.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    review = re.sub('^a-zA-Z^',' ',test['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    tagged.append(tagger.tag(review))\n",
    "    review = ' '.join(review)\n",
    "    corpus_test_stem.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_stem = cv.fit_transform(corpus_train_stem).toarray()\n",
    "X_test_stem = cv.transform(corpus_test_stem).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lr = LogisticRegression(random_state =0)\n",
    "lr.fit(X_train_stem,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)\n",
    "knn.fit(X_train_stem,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classifier = SVC(kernel = 'rbf' , random_state=0)\n",
    "classifier.fit(X_train_stem,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "naive = GaussianNB()\n",
    "naive.fit(X_train_stem,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dtc = DecisionTreeClassifier(criterion = 'entropy', random_state =0)\n",
    "dtc.fit(X_train_stem,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100 , criterion='entropy', random_state=0)\n",
    "rfc.fit(X_train_stem,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:09:42] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_stem,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test_stem)\n",
    "y_pred_knn = knn.predict(X_test_stem)\n",
    "y_pred_classifier = classifier.predict(X_test_stem)\n",
    "y_pred_naive = naive.predict(X_test_stem)\n",
    "y_pred_dtc = dtc.predict(X_test_stem)\n",
    "y_pred_rfc = rfc.predict(X_test_stem)\n",
    "y_pred_xgb = xgb.predict(X_test_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : \n",
      " [[241 105]\n",
      " [ 85 284]]\n",
      "K-NN : \n",
      " [[231 115]\n",
      " [197 172]]\n",
      "SVM :  [[246 100]\n",
      " [ 97 272]]\n",
      "Naive Bayes : \n",
      " [[123 223]\n",
      " [ 97 272]]\n",
      "Decision Tree Classifier : \n",
      " [[185 161]\n",
      " [130 239]]\n",
      "Random Forest Classifier : \n",
      " [[195 151]\n",
      " [ 60 309]]\n",
      "XGBoost Classifier : \n",
      " [[222 124]\n",
      " [ 94 275]]\n",
      "Accuracy of Logistic Regression Classifier is : 0.7342657342657343 \n",
      "\n",
      "Accuracy of K_NN Classifier is : 0.5636363636363636 \n",
      "\n",
      "Accuracy of SVM Classifier is : 0.7244755244755244 \n",
      "\n",
      "Accuracy of Naive Bayes Classifier is : 0.5524475524475524 \n",
      "\n",
      "Accuracy of Decision Tree Classifier is : 0.593006993006993 \n",
      "\n",
      "Accuracy of Random Forest Classifier is : 0.7048951048951049 \n",
      "\n",
      "Accuracy of XGBoost Classifier is : 0.6951048951048951 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cm_lr = confusion_matrix(y_test,y_pred_lr)\n",
    "print('Logistic Regression : \\n',cm_lr)\n",
    "cm_knn = confusion_matrix(y_test,y_pred_knn)\n",
    "print('K-NN : \\n',cm_knn)\n",
    "cm_classifier = confusion_matrix(y_test,y_pred_classifier)\n",
    "print('SVM : ',cm_classifier)\n",
    "cm_naive = confusion_matrix(y_test,y_pred_naive)\n",
    "print('Naive Bayes : \\n',cm_naive)\n",
    "cm_dtc = confusion_matrix(y_test,y_pred_dtc)\n",
    "print('Decision Tree Classifier : \\n',cm_dtc)\n",
    "cm_rfc = confusion_matrix(y_test,y_pred_rfc)\n",
    "print('Random Forest Classifier : \\n', cm_rfc)\n",
    "cm_xgb = confusion_matrix(y_test,y_pred_xgb)\n",
    "print('XGBoost Classifier : \\n', cm_xgb)\n",
    "\n",
    "print('Accuracy of Logistic Regression Classifier is :', accuracy_score(y_test,y_pred_lr),'\\n')\n",
    "print('Accuracy of K_NN Classifier is :', accuracy_score(y_test,y_pred_knn),'\\n')\n",
    "print('Accuracy of SVM Classifier is :' ,accuracy_score(y_test,y_pred_classifier),'\\n')\n",
    "print('Accuracy of Naive Bayes Classifier is :', accuracy_score(y_test,y_pred_naive),'\\n')\n",
    "print('Accuracy of Decision Tree Classifier is :',accuracy_score(y_test,y_pred_dtc),'\\n')\n",
    "print('Accuracy of Random Forest Classifier is :' ,accuracy_score(y_test,y_pred_rfc),'\\n')\n",
    "print('Accuracy of XGBoost Classifier is :' ,accuracy_score(y_test,y_pred_xgb),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
