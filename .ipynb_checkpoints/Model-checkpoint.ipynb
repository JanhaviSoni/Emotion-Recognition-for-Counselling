{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Fine Grained Emotion Recognition for Counselling/dreaddit-train.csv')\n",
    "test = pd.read_csv('../Fine Grained Emotion Recognition for Counselling/dreaddit-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = train['text'].values\n",
    "text_test = test['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "tagger=PerceptronTagger()\n",
    "wordnet = WordNetLemmatizer()\n",
    "corpus = []\n",
    "tagged = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    review = re.sub('^a-zA-Z^',' ',train['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    tagged.append(tagger.tag(review))\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey r/assistance, sure right place post this.. go =) i'm currently student intern sandia national lab working survey help improve marketing outreach effort many school recruit around country. we're looking current undergrad/grad stem student stem student know stem students, would greatly appreciate help take pas along short survey. thank you, everyone help take survey entered drawing chance win one three $50 amazon gcs. \n",
      " \n",
      " mom hit newspaper shocked would this, know like play hitting, smacking, striking, hitting violence sort person. send vibe asking universe? yesterday decided take friend go help another \"friend\" move new place. driving friend moving strike shoulder. address immediately 4th time told things, friend driving nearly get collision another car think high marijuana friend moving backseat like \"you understand trying get attention\" know thing 5 year old get people attention smacking them, guy 60's.\n"
     ]
    }
   ],
   "source": [
    "print(corpus[1],'\\n','\\n',corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('said', 'VBD') \n",
      " ('felt', 'JJ') \n",
      " ('way', 'NN') \n",
      " ('before,', 'NN') \n",
      " ('suggeted', 'VBD')\n"
     ]
    }
   ],
   "source": [
    "print(tagged[0][0],'\\n',tagged[0][1],'\\n',tagged[0][2],'\\n',tagged[0][3],'\\n',tagged[0][4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "sentences_train = []\n",
    "sentences_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data for Word2Vec in training dataset\n",
    "for i in range(len(train)):\n",
    "    review = re.sub('^a-zA-Z^',' ',train['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [word for word in review if word not in set(stopwords.words('english'))]\n",
    "    sentences_train.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data for Word2Vec in testing dataset\n",
    "for i in range(len(test)):\n",
    "    review = re.sub('^a-zA-Z^',' ',test['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [word for word in review if word not in set(stopwords.words('english'))]\n",
    "    sentences_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Word2Vec\n",
    "train_model = Word2Vec(sentences_train, min_count=1)# if word is present less than 1 skip that word\n",
    "test_model = Word2Vec(sentences_test, min_count=1)# if word is present less than 1 skip that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train = train_model.build_vocab(sentences_train)\n",
    "words_test = test_model.build_vocab(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29221292, 30407377)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training word2vec\n",
    "train_model.train(sentences_train,total_examples=train_model.corpus_count,epochs=1001)# corpus_count =2838\n",
    "test_model.train(sentences_test,total_examples=test_model.corpus_count,epochs=1001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
